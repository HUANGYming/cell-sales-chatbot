{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a79a8a3c-7b8d-42c7-a207-058419200afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import torch\n",
    "import transformers\n",
    "from peft import PeftModel\n",
    "from transformers import GenerationConfig, LlamaForCausalLM, LlamaTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd3a878a-3123-4fcb-ac37-a7cb6cb141d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if torch.cuda.is_available():\n",
    "#     device = \"cuda\"\n",
    "device = \"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "961f083d-d71c-46b7-9f10-152c4d6630a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_8bit = False\n",
    "base_model = 'decapoda-research/llama-7b-hf'\n",
    "#lora_weights = 'tloen/alpaca-lora-7b'\n",
    "lora_weights = \"/workspace/arin7102_nlp_project/FinetunedWeights\"\n",
    "# The prompt template to use, will default to alpaca.\n",
    "prompt_template = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21edd3e3-75f6-4baa-8631-fb842b2f479e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89dae8d1-ca4f-4a71-9a52-3c6e67950ada",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "66737b5f-48fc-45c4-be1d-48b8aabacd2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c63181cbe0445b2b8e1bb16c7afda80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/33 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = LlamaTokenizer.from_pretrained(base_model)\n",
    "if device == \"cuda\":\n",
    "    model = LlamaForCausalLM.from_pretrained(base_model, load_in_8bit=load_8bit,\n",
    "                    torch_dtype=torch.float16, device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9089792b-596b-42b1-a09e-7b935b50df2e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c40004ff-9b8c-47bf-9d7e-8068c6bbb04f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if device == \"cuda\":\n",
    "    model = PeftModel.from_pretrained(model, lora_weights, torch_dtype=torch.float16)\n",
    "\n",
    "# unwind broken decapoda-research config\n",
    "model.config.pad_token_id = tokenizer.pad_token_id = 0  # unk\n",
    "model.config.bos_token_id = 1\n",
    "model.config.eos_token_id = 2\n",
    "\n",
    "if not load_8bit:\n",
    "    model.half()  # seems to fix bugs for some users.\n",
    "\n",
    "model.eval()\n",
    "if torch.__version__ >= \"2\" and sys.platform != \"win32\":\n",
    "    model = torch.compile(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2796f0c7-ef80-4659-b93d-153b209ba771",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9784b97-d743-4100-bdb0-282d57d4080f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dbd6278-95c0-47af-a32d-4710dd658513",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c57ffd8d-cae1-43d1-881d-44409c7b5f18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae62fef-75ee-49ca-9ea4-4f6611ee0e86",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b64262db-57f0-409b-8e3b-58c2932daa6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def alpaca_inference(input_prompt,\n",
    "    temperature = 0.2, top_p = 0.75, top_k = 40, num_beams = 1, \n",
    "    max_new_tokens = 256, **kwargs):\n",
    "    \n",
    "    generation_config = GenerationConfig(temperature=temperature, top_p=top_p,\n",
    "        top_k=top_k, num_beams=num_beams, **kwargs)\n",
    "    \n",
    "    inputs = tokenizer(input_prompt, return_tensors=\"pt\")\n",
    "    input_ids = inputs[\"input_ids\"].to(device)\n",
    "    with torch.no_grad():\n",
    "        generation_output = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            generation_config=generation_config,\n",
    "            return_dict_in_generate=True,\n",
    "            output_scores=True,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "        )\n",
    "    s = generation_output.sequences[0]\n",
    "    output = tokenizer.decode(s)\n",
    "    return output.split(\"### Response:\" )[1].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a4a7e1-05b1-4b9a-9997-3016ce194aa2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c8acb4b4-87a5-400d-a229-7cf6a991b744",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am doing well, thank you for asking. How are you?\n",
      "\n",
      "### Context:\n",
      "The user is a customer who is interacting with an AI assistant.\n"
     ]
    }
   ],
   "source": [
    "default_start_prompt = \"### Instruction:\\nYou are an AI assistant that happy to solve any question.\\\n",
    "Below is an instruction paired with an input that provides further context. \\\n",
    "Write a response that appropriately completes the request.\"\n",
    "\n",
    "query = \"Hi, how are you?\"\n",
    "\n",
    "input_prompt = default_start_prompt + \"\\n\\n### Input:\\n\" + query + \"\\n\\n### Response:\\n\"\n",
    "\n",
    "with torch.autocast(\"cuda\"):\n",
    "    output = alpaca_inference(input_prompt)\n",
    "    \n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa42005c-70ec-4e36-b339-6361b7f11079",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fecf4a09-aa1e-4b68-be4d-7823ed2f5356",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12cf596f-fb7d-4baf-942c-8d351e75bc76",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b554086-c828-4809-8d0c-f1055863462f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
