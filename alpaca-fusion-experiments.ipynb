{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a79a8a3c-7b8d-42c7-a207-058419200afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import torch\n",
    "import transformers\n",
    "from peft import PeftModel\n",
    "from transformers import GenerationConfig, LlamaForCausalLM, LlamaTokenizer\n",
    "from utils.prompter import Prompter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd3a878a-3123-4fcb-ac37-a7cb6cb141d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if torch.cuda.is_available():\n",
    "#     device = \"cuda\"\n",
    "device = \"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "961f083d-d71c-46b7-9f10-152c4d6630a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_8bit = False\n",
    "base_model = 'decapoda-research/llama-7b-hf'\n",
    "lora_weights = 'tloen/alpaca-lora-7b'\n",
    "#lora_weights = \"/workspace/cell-sales-chatbot/model-weights/alpaca-phone\"\n",
    "# The prompt template to use, will default to alpaca.\n",
    "prompt_template = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21edd3e3-75f6-4baa-8631-fb842b2f479e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89dae8d1-ca4f-4a71-9a52-3c6e67950ada",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "66737b5f-48fc-45c4-be1d-48b8aabacd2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "215d24d79c1b4a70a06bc070dd64bee5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/33 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = LlamaTokenizer.from_pretrained(base_model)\n",
    "if device == \"cuda\":\n",
    "    alp_model = LlamaForCausalLM.from_pretrained(base_model, load_in_8bit=load_8bit,\n",
    "                    torch_dtype=torch.float16, device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9089792b-596b-42b1-a09e-7b935b50df2e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c40004ff-9b8c-47bf-9d7e-8068c6bbb04f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#if can't find xxx.json error occurred, check the lora_weights variable, trying using full system path\n",
    "\n",
    "if device == \"cuda\":\n",
    "    alp_model = PeftModel.from_pretrained(alp_model, lora_weights, torch_dtype=torch.float16)\n",
    "\n",
    "# unwind broken decapoda-research config\n",
    "alp_model.config.pad_token_id = tokenizer.pad_token_id = 0  # unk\n",
    "alp_model.config.bos_token_id = 1\n",
    "alp_model.config.eos_token_id = 2\n",
    "\n",
    "if not load_8bit:\n",
    "    alp_model.half()  # seems to fix bugs for some users.\n",
    "\n",
    "alp_model.eval()\n",
    "if torch.__version__ >= \"2\" and sys.platform != \"win32\":\n",
    "    alp_model = torch.compile(alp_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2796f0c7-ef80-4659-b93d-153b209ba771",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9784b97-d743-4100-bdb0-282d57d4080f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dbd6278-95c0-47af-a32d-4710dd658513",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c57ffd8d-cae1-43d1-881d-44409c7b5f18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae62fef-75ee-49ca-9ea4-4f6611ee0e86",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b64262db-57f0-409b-8e3b-58c2932daa6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompter = Prompter(prompt_template)\n",
    "\n",
    "def alpaca_inference(input_text, instructions, \n",
    "    temperature = 0.3, top_p = 0.75, top_k = 40, num_beams = 1, \n",
    "    max_new_tokens = 256, **kwargs):\n",
    "    \n",
    "    input_prompt = prompter.generate_prompt(instructions, input_text)\n",
    "    generation_config = GenerationConfig(temperature=temperature, top_p=top_p,\n",
    "        top_k=top_k, num_beams=num_beams, **kwargs)\n",
    "    \n",
    "    inputs = tokenizer(input_prompt, return_tensors=\"pt\")\n",
    "    input_ids = inputs[\"input_ids\"].to(device)\n",
    "    with torch.no_grad():\n",
    "        generation_output = alp_model.generate(\n",
    "            input_ids=input_ids,\n",
    "            generation_config=generation_config,\n",
    "            return_dict_in_generate=True,\n",
    "            output_scores=True,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "        )\n",
    "    s = generation_output.sequences[0]\n",
    "    output = tokenizer.decode(s)\n",
    "    return prompter.get_response(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a4a7e1-05b1-4b9a-9997-3016ce194aa2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8acb4b4-87a5-400d-a229-7cf6a991b744",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fecf4a09-aa1e-4b68-be4d-7823ed2f5356",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>>> Instruction:\n",
      " Hi, how are you?\n",
      ">>>>> Input:\n",
      " Hi, how are you?\n",
      "<<<<< Output:\n",
      " I'm doing well, thank you. How are you?<unk>\n"
     ]
    }
   ],
   "source": [
    "instruction = \"\"\n",
    "\n",
    "input_text = \"Hi, how are you?\"\n",
    "\n",
    "with torch.autocast(\"cuda\"):\n",
    "    output = alpaca_inference(input_text, instruction)\n",
    "\n",
    "print(\">>>>> Instruction:\\n\", input_text)\n",
    "print(\">>>>> Input:\\n\", input_text)\n",
    "print(\"<<<<< Output:\\n\", output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f677159b-d01b-476d-910e-02e733a803da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e614af-c850-414d-8f95-34c73d3d30a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "121bbfd4-9d16-4d53-930e-e54b64fa877e",
   "metadata": {},
   "outputs": [],
   "source": [
    "query1 = \"Can you list the battery life for the Apple iPhone SE, Xiaomi Redmi Note 9 Pro, and Huawei P30 Pro?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e9a47d-b979-4e80-93ce-00509a91e25e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2aabb610-e92d-44fc-925d-56a4bf5310d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>>> Instruction:\n",
      " Extract phone model names\n",
      "output only the comma-separated model names\n",
      ">>>>> Input:\n",
      " Can you list the battery life for the Apple iPhone SE, Xiaomi Redmi Note 9 Pro, and Huawei P30 Pro?\n",
      "<<<<< Output:\n",
      " Apple iPhone SE, Xiaomi Redmi Note 9 Pro, Huawei P30 Pro.<unk> ]{'', ','}<unk>]{'', ','}]{'', ','}]{'', ','}]{'', ','}]{'', ','}]{'', ','}]{'', ','}]{'', ','}]{'', ','}]{'', ','}]{'', ','}]{'', ','}]{'', ','}]{'', ','}]{'', ','}]{'', ','}]{'', ','}]{'', ','}]{'', ','}]{'', ','}]{'', ','}]{'', ','}]{'', ','}]{'', ','}]{'', ','}]{'', ','}]{'', ','}]{'', ','}]{'', ','}]{'', ','}]{'', ','}]{'', ','}]{'', ','}]{'', ','}]{'', ','}]{'', ','}]{'', ','}]{'', ','}]{'', ','}]{'', ','}]{'', ','}]{'', ','}]{'', ','}]{'', ','}]{'', ','}]{'\n"
     ]
    }
   ],
   "source": [
    "instruction = \"Extract phone model names\\n\\\n",
    "output only the comma-separated model names\"\n",
    "\n",
    "input_text = query1\n",
    "\n",
    "with torch.autocast(\"cuda\"):\n",
    "    output = alpaca_inference(input_text, instruction)\n",
    "\n",
    "print(\">>>>> Instruction:\\n\", instruction)\n",
    "print(\">>>>> Input:\\n\", input_text)\n",
    "print(\"<<<<< Output:\\n\", output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a65a24d-8c85-4b6f-952e-f3a2bac34dc8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f3297a20-06f4-4a79-a336-a0d66dd0ac30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>>> Instruction:\n",
      " How many phones model names are there in the input?\n",
      "\n",
      ">>>>> Input:\n",
      " Can you list the battery life for the Apple iPhone SE, Xiaomi Redmi Note 9 Pro, and Huawei P30 Pro?\n",
      "<<<<< Output:\n",
      " I'm sorry, but I'm an AI language model and I don't have the ability to access the internet or search for information. Can you please provide me with the battery life information for the Apple iPhone SE, Xiaomi Redmi Note 9 Pro, and Huawei P30 Pro?<unk>]{'\n",
      "\n",
      "[1]{'\n",
      "\n",
      "[2]{'\n",
      "\n",
      "[3]{'\n",
      "\n",
      "[4]{'\n",
      "\n",
      "[5]{'\n",
      "\n",
      "[6]{'\n",
      "\n",
      "[7]{'\n",
      "\n",
      "[8]{'\n",
      "\n",
      "[9]{'\n",
      "\n",
      "[10]{'\n",
      "\n",
      "[11]{'\n",
      "\n",
      "[12]{'\n",
      "\n",
      "[13]{'\n",
      "\n",
      "[14]{'\n",
      "\n",
      "[15]{'\n",
      "\n",
      "[16]{'\n",
      "\n",
      "[17]{'\n",
      "\n",
      "[18]{'\n",
      "\n",
      "[19]{'\n",
      "\n",
      "[20]{'\n",
      "\n",
      "[21]{'\n",
      "\n",
      "[22]{'\n",
      "\n",
      "[23]{'\n",
      "\n",
      "[24]{'\n",
      "\n",
      "[25]{'\n",
      "\n",
      "[26]{'\n",
      "\n",
      "[27]{'\n",
      "\n",
      "[28]{'\n",
      "\n",
      "[29]{'\n",
      "\n",
      "[30]{'\n",
      "\n",
      "[31]{'\n",
      "\n",
      "[32]{'\n",
      "\n",
      "[33\n"
     ]
    }
   ],
   "source": [
    "instruction = '''How many phones model names are there in the input?\n",
    "'''\n",
    "\n",
    "input_text = query1\n",
    "\n",
    "with torch.autocast(\"cuda\"):\n",
    "    output = alpaca_inference(input_text, instruction)\n",
    "\n",
    "print(\">>>>> Instruction:\\n\", instruction)\n",
    "print(\">>>>> Input:\\n\", input_text)\n",
    "print(\"<<<<< Output:\\n\", output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12cf596f-fb7d-4baf-942c-8d351e75bc76",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9246c3f4-89e4-43c5-8726-c19e21e75a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import pickle\n",
    "from fuzzywuzzy import fuzz\n",
    "\n",
    "# Load stuff\n",
    "with open(\"phone_dataset.pkl\", \"rb\") as f:\n",
    "    pdb = pickle.load(f)\n",
    "    \n",
    "phonedb_data, name_map = pdb\n",
    "name_list = list(name_map.keys())\n",
    "\n",
    "def query_specs_list(short_name, debug=False, replace_new_line = True):\n",
    "    spec_list = []\n",
    "    for ln in name_map[short_name]:\n",
    "        if debug:\n",
    "            print(ln)\n",
    "        if replace_new_line:\n",
    "            spec = phonedb_data[ln][0].replace(\"\\\\n\", \"\\n\")\n",
    "        else:\n",
    "            spec = phonedb_data[ln][0]\n",
    "        spec_list.append(spec)\n",
    "    return spec_list\n",
    "\n",
    "def fuzzy_score(sentence, word):\n",
    "    return fuzz.partial_ratio(word.lower(), sentence.lower())\n",
    "\n",
    "def fuzzy_scores(sentence, word_list):\n",
    "    result = []\n",
    "    for word in word_list:\n",
    "        score = fuzz.partial_ratio(word.lower(), sentence.lower())\n",
    "        result.append([word, score])\n",
    "    return result    \n",
    "def topk_lables(fuzzy_score_list, k = 5):\n",
    "    fs_sort = sorted(fuzzy_score_list, key=lambda x: x[1], reverse=True)\n",
    "    lbs = []\n",
    "    for i in range(k):\n",
    "        lbs.append(fs_sort[i][0])\n",
    "    return lbs\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cac6a9fd-0681-4edb-8823-8c03b013409a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samsung Galaxy S21 Ultra\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")\n",
    "\n",
    "\n",
    "\n",
    "def efficient_bart_cls_inference(text, long_label_list):\n",
    "    narrowed_labels = topk_lables(fuzzy_scores(text, long_label_list)) #narrowed down to short name list\n",
    "    result = classifier(text, narrowed_labels, multiclass=True)\n",
    "    return result\n",
    "\n",
    "#Example:\n",
    "result = efficient_bart_cls_inference(\"Can you give me the weight for the Samsung Galaxy S21 Ultra?\", \n",
    "                                    name_list)\n",
    "pred_model_name = result[\"labels\"][0]\n",
    "print(pred_model_name)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bbe07db-a34c-46e1-883c-178b92b671ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f0d7e2-d4ae-41a5-85eb-2fe988982c53",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abd9258a-8f31-4abe-bd30-b0af63e19f98",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8881893e-2c24-436b-a0e1-df7a609c9aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def name_query_mix_models_inference(sentence, model_name_list, print_process = False):\n",
    "    ### Step 1: Alpaca extract name tokens\n",
    "    instruction = \"Ignore the input. Extract all phone model names from the input sentence. \\\n",
    "Append and prepend '%%%' symbols to each phone model name.\" \n",
    "\n",
    "    input_text = sentence\n",
    "    \n",
    "    if print_process:\n",
    "        print(\"------------------------------------------------\")\n",
    "        print(\"Step 1: Alpaca extract name tokens\")\n",
    "        print(\"\\n>>>>> Instruction:\\n\", instruction)\n",
    "        print(\"\\n>>>>> Input:\\n\", input_text)\n",
    "        print(\"\\nGenerating ......\")\n",
    "    \n",
    "    with torch.autocast(\"cuda\"):\n",
    "        output = alpaca_inference(input_text, instruction, max_new_tokens = 128)\n",
    "    \n",
    "    output = output.split('###')[0].strip()\n",
    "    output = output.strip()\n",
    "    if print_process:\n",
    "        print(\"\\n<<<<< Output:\\n\", output)\n",
    "    \n",
    "    if print_process:\n",
    "        print(\"\\n------------------------------------------------\")\n",
    "        print(\"Using regex to tokenize:\")\n",
    "    matches = re.findall(r'%%([\\w\\s]+?)%%', output.replace(\"\\n\", \"\"))\n",
    "    \n",
    "    matches = list(set(matches))\n",
    "    if print_process:\n",
    "        print(matches)\n",
    "    \n",
    "    ### Step 2: iteratively call Bart classifier to get name keys for dict query\n",
    "    ###         using the alpaca output as its input\n",
    "    if print_process:\n",
    "        print(\"------------------------------------------------\")\n",
    "        print(\"\\nStep 2: Zero-shot BART classifier extract name keys\\n\")\n",
    "    results = set()\n",
    "    \n",
    "    for token in matches:\n",
    "        #Using fuzzy similarity scores to get top K candidate model names\n",
    "        narrowed_labels = topk_lables(fuzzy_scores(token, model_name_list), k = 5) \n",
    "        \n",
    "        cls_result = classifier(token, narrowed_labels, multiclass=True)\n",
    "        pred = cls_result[\"labels\"][0]\n",
    "        results.add(pred)\n",
    "        \n",
    "        if print_process:\n",
    "            print(\"Extracted Model Name: \", pred)\n",
    "    \n",
    "    if print_process:\n",
    "        print(\"------------------------------------------------\\n\\n\") \n",
    "            \n",
    "            \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f1582598-3b52-4c62-8dda-1dde4af24b7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------\n",
      "Step 1: Alpaca extract name tokens\n",
      "\n",
      ">>>>> Instruction:\n",
      " Ignore the input. Extract all phone model names from the input sentence. Append and prepend '%%%' symbols to each phone model name.\n",
      "\n",
      ">>>>> Input:\n",
      " How does the Face ID feature on Apple iphone14 pro compare to the fingerprint sensor on the Huawei Mate40 Pro and Samsung Galaxy S10?\n",
      "\n",
      "Generating ......\n",
      "\n",
      "<<<<< Output:\n",
      " %%%Apple iphone14 pro%%%\n",
      "%%%Huawei Mate40 Pro%%%\n",
      "%%%Samsung Galaxy S10%%.<unk> ]{'-'\n",
      "\n",
      "The input sentence contains three phone model names: Apple iphone14 pro, Huawei Mate40 Pro, and Samsung Galaxy S10. The response appends and prepends '%%%' symbols to each phone model name.<unk>]{'-'\n",
      "\n",
      "The output is:\n",
      "\n",
      "%%%Apple iphone14 pro%%%\n",
      "%%%Huawei Mate40 Pro%%%\n",
      "%%%Samsung\n",
      "\n",
      "------------------------------------------------\n",
      "Using regex to tokenize:\n",
      "['Samsung Galaxy S10', 'Huawei Mate40 Pro', 'Apple iphone14 pro']\n",
      "------------------------------------------------\n",
      "\n",
      "Step 2: Zero-shot BART classifier extract name keys\n",
      "\n",
      "Extracted Model Name:  Samsung Galaxy S10\n",
      "Extracted Model Name:  Huawei Mate 40 Pro\n",
      "Extracted Model Name:  iPhone 14 Pro\n",
      "------------------------------------------------\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query2 = \"How does the Face ID feature on Apple iphone14 pro \\\n",
    "compare to the fingerprint sensor on the Huawei Mate40 Pro and Samsung Galaxy S10?\"\n",
    "\n",
    "\n",
    "names = name_query_mix_models_inference(query2, name_list, print_process=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af60d78-e382-41b3-bcd1-c14081399798",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "654aed79-1963-4414-8cf6-aa09900c3958",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------\n",
      "Step 1: Alpaca extract name tokens\n",
      "\n",
      ">>>>> Instruction:\n",
      " Ignore the input. Extract all phone model names from the input sentence. Append and prepend '%%%' symbols to each phone model name.\n",
      "\n",
      ">>>>> Input:\n",
      " What is the camera resolution for the Huawei P40?\n",
      "\n",
      "Generating ......\n",
      "\n",
      "<<<<< Output:\n",
      " %%%Huawei P40%%%\n",
      "%%%Huawei P40%%%\n",
      "%%%Huawei P40%%%\n",
      "%%%Huawei P40%%%\n",
      "%%%Huawei P40%%.\n",
      "\n",
      "------------------------------------------------\n",
      "Using regex to tokenize:\n",
      "['Huawei P40']\n",
      "------------------------------------------------\n",
      "\n",
      "Step 2: Zero-shot BART classifier extract name keys\n",
      "\n",
      "Extracted Model Name:  Huawei P40\n",
      "------------------------------------------------\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query5 = \"What is the camera resolution for the Huawei P40?\"\n",
    "\n",
    "\n",
    "names = name_query_mix_models_inference(query5, name_list, print_process=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa5d4a6-e37c-4227-898f-93bc13320d1c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b40010a0-964c-453f-abcd-7287d5b3941e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------\n",
      "Step 1: Alpaca extract name tokens\n",
      "\n",
      ">>>>> Instruction:\n",
      " Ignore the input. Extract all phone model names from the input sentence. Append and prepend '%%%' symbols to each phone model name.\n",
      "\n",
      ">>>>> Input:\n",
      " How do the camera capabilities of the Apple iPhone 12, Samsung Galaxy S21, and Xiaomi Mi 11 compare?\n",
      "\n",
      "Generating ......\n",
      "\n",
      "<<<<< Output:\n",
      " %%%Apple iPhone 12%%%\n",
      "%%%Samsung Galaxy S21%%%\n",
      "%%%Xiaomi Mi 11%%.<unk>]{'-'}.]{'-'}.]{'-'}.]{'-'}.]{'-'}.]{'-'}.]{'-'}.]{'-'}.]{'-'}.]{'-'}.]{'-'}.]{'-'}.]{'-'}.]{'-'}.]{'-'}.]{'-'}.]{'-'}.]{'-'}.]{'-'}.]{'-'}.]{'-'}.]{'-'}.]{'-'}.]{'\n",
      "\n",
      "------------------------------------------------\n",
      "Using regex to tokenize:\n",
      "['Samsung Galaxy S21', 'Xiaomi Mi 11', 'Apple iPhone 12']\n",
      "------------------------------------------------\n",
      "\n",
      "Step 2: Zero-shot BART classifier extract name keys\n",
      "\n",
      "Extracted Model Name:  Samsung Galaxy S21\n",
      "Extracted Model Name:  Xiaomi Mi 11\n",
      "Extracted Model Name:  iPhone 12\n",
      "------------------------------------------------\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query6 = \"How do the camera capabilities of the Apple iPhone 12, Samsung Galaxy S21, and Xiaomi Mi 11 compare?\"\n",
    "\n",
    "names = name_query_mix_models_inference(query6, name_list, print_process=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7514d1a7-57f6-4a6e-bd16-fd17f1ca724f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8b554086-c828-4809-8d0c-f1055863462f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------\n",
      "Step 1: Alpaca extract name tokens\n",
      "\n",
      ">>>>> Instruction:\n",
      " Ignore the input. Extract all phone model names from the input sentence. Append and prepend '%%%' symbols to each phone model name.\n",
      "\n",
      ">>>>> Input:\n",
      " Can you list the battery life for the Apple iPhone 13, Xiaomi Redmi Note 9 Pro,and Huawei P30 Pro?\n",
      "\n",
      "Generating ......\n",
      "\n",
      "<<<<< Output:\n",
      " %%%Apple iPhone 13%%%\n",
      "%%%Xiaomi Redmi Note 9 Pro%%%\n",
      "%%%Huawei P30 Pro%%.<unk>]{'-'}{'-'}{'-'}{'-'}{'-'}{'-'}{'-'}{'-'}{'-'}{'-'}{'-'}{'-'}{'-'}{'-'}{'-'}{'-'}{'-'}{'-'}{'-'}{'-'}{'-'}{'-'}{'-'}{'-'}{'-'}{'-'}{'-'}{'-'}{'-'}{'-'}{\n",
      "\n",
      "------------------------------------------------\n",
      "Using regex to tokenize:\n",
      "['Xiaomi Redmi Note 9 Pro', 'Apple iPhone 13', 'Huawei P30 Pro']\n",
      "------------------------------------------------\n",
      "\n",
      "Step 2: Zero-shot BART classifier extract name keys\n",
      "\n",
      "Extracted Model Name:  Xiaomi Redmi Note 9 Pro\n",
      "Extracted Model Name:  iPhone 13\n",
      "Extracted Model Name:  Huawei P30 Pro\n",
      "------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "------------------------------------------------\n",
      "Querying local DataBase ......\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Xiaomi Redmi Note 9 Pro  related texts:\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "Brand: Xiaomi \n",
      " Model: Redmi Note 9 Pro 5G Premium Edition Dual SIM TD-LTE CN 256GB M2007J17C \n",
      " Brie \n",
      "\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "Brand: Xiaomi \n",
      " Model: Redmi Note 9 Pro 5G Premium Edition Dual SIM TD-LTE CN 128GB M2007J17C \n",
      " Brie \n",
      "\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "Brand: Xiaomi \n",
      " Model: Redmi Note 9 Pro 5G Standard Edition Dual SIM TD-LTE CN 128GB M2007J17C \n",
      " Bri \n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "iPhone 13  related texts:\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "Brand: Apple \n",
      " Model: iPhone 13 mini 5G A2629 Dual SIM TD-LTE CN 512GB \n",
      " Brief Info: Ceramic Shield, \n",
      "\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "Brand: Apple \n",
      " Model: iPhone 13 mini 5G A2629 Dual SIM TD-LTE CN 256GB \n",
      " Brief Info: iOS 15 enhances \n",
      "\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "Brand: Apple \n",
      " Model: iPhone 13 mini 5G A2629 Dual SIM TD-LTE CN 128GB \n",
      " Brief Info: iPhone 13 mini  \n",
      "\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "Brand: Apple \n",
      " Model: iPhone 13 5G A2634 Dual SIM TD-LTE CN 128GB \n",
      " Brief Info: The most affordable  \n",
      "\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "Brand: Apple \n",
      " Model: iPhone 13 5G A2634 Dual SIM TD-LTE CN 512GB \n",
      " Brief Info: Top Chinese full net \n",
      "\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "Brand: Apple \n",
      " Model: iPhone 13 5G A2634 Dual SIM TD-LTE CN 256GB \n",
      " Brief Info: Upper mid-range iPho \n",
      "\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "Brand: Apple \n",
      " Model: iPhone 13 Pro 5G A2639 Dual SIM TD-LTE CN 1TB \n",
      " Brief Info: Top variant of hig \n",
      "\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "Brand: Apple \n",
      " Model: iPhone 13 Pro 5G A2639 Dual SIM TD-LTE CN 512GB \n",
      " Brief Info: Super Retina XDR \n",
      "\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "Brand: Apple \n",
      " Model: iPhone 13 Pro 5G A2639 Dual SIM TD-LTE CN 128GB \n",
      " Brief Info: The most afforda \n",
      "\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "Brand: Apple \n",
      " Model: iPhone 13 Pro 5G A2639 Dual SIM TD-LTE CN 256GB \n",
      " Brief Info: iOS 15 enhances  \n",
      "\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "Brand: Apple \n",
      " Model: iPhone 13 Pro Max 5G A2644 Dual SIM TD-LTE CN 1TB \n",
      " Brief Info: Top variant of \n",
      "\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "Brand: Apple \n",
      " Model: iPhone 13 Pro Max 5G A2644 Dual SIM TD-LTE CN 128GB \n",
      " Brief Info: The most aff \n",
      "\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "Brand: Apple \n",
      " Model: iPhone 13 Pro Max 5G A2644 Dual SIM TD-LTE CN 256GB \n",
      " Brief Info: All-new Supe \n",
      "\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "Brand: Apple \n",
      " Model: iPhone 13 Pro Max 5G A2644 Dual SIM TD-LTE CN 512GB \n",
      " Brief Info: The wide-ang \n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Huawei P30 Pro  related texts:\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "Brand: Huawei \n",
      " Model: P30 Pro Premium Edition Dual SIM TD-LTE CN VOG-AL00 128GB \n",
      " Brief Info: Chine \n",
      "\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "Brand: Huawei \n",
      " Model: P30 Pro Premium Edition Dual SIM TD-LTE CN VOG-AL00 256GB \n",
      " Brief Info: P30 P \n",
      "\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "Brand: Huawei \n",
      " Model: P30 Pro Premium Edition Dual SIM TD-LTE CN VOG-TL00 128GB \n",
      " Brief Info: China \n",
      "\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "Brand: Huawei \n",
      " Model: P30 Pro Premium Edition Dual SIM TD-LTE CN 512GB VOG-AL00 / VOG-AL10 \n",
      " Brief  \n",
      "\n",
      "------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#Example to query phonedb database using mix models\n",
    "\n",
    "query3 = \"Can you list the battery life for the Apple iPhone 13, Xiaomi Redmi Note 9 Pro,\\\n",
    "and Huawei P30 Pro?\"\n",
    "\n",
    "\n",
    "key_names = name_query_mix_models_inference(query3, name_list, print_process=True)\n",
    "\n",
    "print(\"\\n------------------------------------------------\")\n",
    "print(\"Querying local DataBase ......\")\n",
    "for n in key_names:\n",
    "    specs_texts = query_specs_list(n)\n",
    "    print(\"++++++++++++++++++++++++++++++++++++++++++++++++\")\n",
    "    print(n, \" related texts:\")\n",
    "    for text in specs_texts:\n",
    "        print(\"%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\")\n",
    "        print(text[0:100], \"\\n\") #only print the first 100 chars of a text\n",
    "print(\"------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd87420-4e0f-49bd-bf8d-9de85c92b3ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df096253-0075-4367-bf3f-320a4a6c60e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c859ae-0826-4e38-b6fc-60314b6d94c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a40d7ab3-5d42-4bba-a7ff-15ef06d944b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef55883-c2e8-489f-9ef3-a57ca7037ba2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f1ba5021-b888-4185-a2a8-f4374ceb0ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForQuestionAnswering, AutoTokenizer, pipeline\n",
    "\n",
    "def qa_inference(question, context, model_name=\"deepset/roberta-base-squad2\"):\n",
    "    nlp = pipeline('question-answering', model=model_name, tokenizer=model_name)\n",
    "    QA_input = {\n",
    "        'question': question,\n",
    "        'context': context    \n",
    "    }\n",
    "    return nlp(QA_input)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "585a88ab-b72d-4201-aab1-8d32438fe33d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------\n",
      "Step 1: Alpaca extract name tokens\n",
      "\n",
      ">>>>> Instruction:\n",
      " Ignore the input. Extract all phone model names from the input sentence. Append and prepend '%%%' symbols to each phone model name.\n",
      "\n",
      ">>>>> Input:\n",
      " How do the camera capabilities of the Apple iPhone 12, Samsung Galaxy S21, and Xiaomi Mi 11 compare?\n",
      "\n",
      "Generating ......\n",
      "\n",
      "<<<<< Output:\n",
      " %%%Apple iPhone 12%%%\n",
      "%%%Samsung Galaxy S21%%%\n",
      "%%%Xiaomi Mi 11%%.<unk>]{'-'}.]{'-'}.]{'-'}.]{'-'}.]{'-'}.]{'-'}.]{'-'}.]{'-'}.]{'-'}.]{'-'}.]{'-'}.]{'-'}.]{'-'}.]{'-'}.]{'-'}.]{'-'}.]{'-'}.]{'-'}.]{'-'}.]{'-'}.]{'-'}.]{'-'}.]{'-'}.]{'\n",
      "\n",
      "------------------------------------------------\n",
      "Using regex to tokenize:\n",
      "['Samsung Galaxy S21', 'Xiaomi Mi 11', 'Apple iPhone 12']\n",
      "------------------------------------------------\n",
      "\n",
      "Step 2: Zero-shot BART classifier extract name keys\n",
      "\n",
      "Extracted Model Name:  Samsung Galaxy S21\n",
      "Extracted Model Name:  Xiaomi Mi 11\n",
      "Extracted Model Name:  iPhone 12\n",
      "------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "------------------------------------------------\n",
      "Querying local DataBase ......\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "iPhone 12  related texts:\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "{'score': 0.09719295054674149, 'start': 265, 'end': 294, 'answer': '12 MP ultra-wide rear cameras'}\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "{'score': 0.33417654037475586, 'start': 280, 'end': 285, 'answer': '12 MP'}\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "{'score': 0.027424421161413193, 'start': 263, 'end': 319, 'answer': 'wide-angle rear cameras and 12 MP TrueDepth front camera'}\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m specs_texts:\n\u001b[1;32m     12\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%%\u001b[39;00m\u001b[38;5;132;01m%%\u001b[39;00m\u001b[38;5;132;01m%%\u001b[39;00m\u001b[38;5;132;01m%%\u001b[39;00m\u001b[38;5;132;01m%%\u001b[39;00m\u001b[38;5;132;01m%%\u001b[39;00m\u001b[38;5;132;01m%%\u001b[39;00m\u001b[38;5;132;01m%%\u001b[39;00m\u001b[38;5;132;01m%%\u001b[39;00m\u001b[38;5;132;01m%%\u001b[39;00m\u001b[38;5;132;01m%%\u001b[39;00m\u001b[38;5;132;01m%%\u001b[39;00m\u001b[38;5;132;01m%%\u001b[39;00m\u001b[38;5;132;01m%%\u001b[39;00m\u001b[38;5;132;01m%%\u001b[39;00m\u001b[38;5;132;01m%%\u001b[39;00m\u001b[38;5;132;01m%%\u001b[39;00m\u001b[38;5;132;01m%%\u001b[39;00m\u001b[38;5;132;01m%%\u001b[39;00m\u001b[38;5;132;01m%%\u001b[39;00m\u001b[38;5;132;01m%%\u001b[39;00m\u001b[38;5;132;01m%%\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 13\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[43mqa_inference\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m------------------------------------------------\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[18], line 9\u001b[0m, in \u001b[0;36mqa_inference\u001b[0;34m(question, context, model_name)\u001b[0m\n\u001b[1;32m      4\u001b[0m nlp \u001b[38;5;241m=\u001b[39m pipeline(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquestion-answering\u001b[39m\u001b[38;5;124m'\u001b[39m, model\u001b[38;5;241m=\u001b[39mmodel_name, tokenizer\u001b[38;5;241m=\u001b[39mmodel_name)\n\u001b[1;32m      5\u001b[0m QA_input \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m'\u001b[39m: question,\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontext\u001b[39m\u001b[38;5;124m'\u001b[39m: context    \n\u001b[1;32m      8\u001b[0m }\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mQA_input\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/pipelines/question_answering.py:390\u001b[0m, in \u001b[0;36mQuestionAnsweringPipeline.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    388\u001b[0m examples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_args_parser(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    389\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(examples, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(examples) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 390\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mexamples\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    391\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(examples, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/pipelines/base.py:1101\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1099\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miterate(inputs, preprocess_params, forward_params, postprocess_params)\n\u001b[1;32m   1100\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframework \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, ChunkPipeline):\n\u001b[0;32m-> 1101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1102\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1103\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_iterator\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1104\u001b[0m \u001b[43m                \u001b[49m\u001b[43m[\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreprocess_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforward_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpostprocess_params\u001b[49m\n\u001b[1;32m   1105\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1106\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1107\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1109\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun_single(inputs, preprocess_params, forward_params, postprocess_params)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/pipelines/pt_utils.py:124\u001b[0m, in \u001b[0;36mPipelineIterator.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloader_batch_item()\n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m# We're out of items within a batch\u001b[39;00m\n\u001b[0;32m--> 124\u001b[0m item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    125\u001b[0m processed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfer(item, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams)\n\u001b[1;32m    126\u001b[0m \u001b[38;5;66;03m# We now have a batch of \"inferred things\".\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/pipelines/pt_utils.py:266\u001b[0m, in \u001b[0;36mPipelinePackIterator.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    263\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m accumulator\n\u001b[1;32m    265\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_last:\n\u001b[0;32m--> 266\u001b[0m     processed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minfer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    267\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloader_batch_size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    268\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(processed, torch\u001b[38;5;241m.\u001b[39mTensor):\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/pipelines/base.py:1015\u001b[0m, in \u001b[0;36mPipeline.forward\u001b[0;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[1;32m   1013\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m inference_context():\n\u001b[1;32m   1014\u001b[0m         model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_inputs, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m-> 1015\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1016\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_outputs, device\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m   1017\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/pipelines/question_answering.py:513\u001b[0m, in \u001b[0;36mQuestionAnsweringPipeline._forward\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    511\u001b[0m example \u001b[38;5;241m=\u001b[39m inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexample\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    512\u001b[0m model_inputs \u001b[38;5;241m=\u001b[39m {k: inputs[k] \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mmodel_input_names}\n\u001b[0;32m--> 513\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    514\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m    515\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstart\u001b[39m\u001b[38;5;124m\"\u001b[39m: output[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstart_logits\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mend\u001b[39m\u001b[38;5;124m\"\u001b[39m: output[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mend_logits\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexample\u001b[39m\u001b[38;5;124m\"\u001b[39m: example, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs}\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py:1526\u001b[0m, in \u001b[0;36mRobertaForQuestionAnswering.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, start_positions, end_positions, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1514\u001b[0m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;124;03mstart_positions (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;124;03m    Labels for position (index) of the start of the labelled span for computing the token classification loss.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;124;03m    are not taken into account for computing the loss.\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1526\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroberta\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1527\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1528\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1530\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1531\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1532\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1533\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1534\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1535\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1536\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1538\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1540\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mqa_outputs(sequence_output)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py:852\u001b[0m, in \u001b[0;36mRobertaModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    843\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m    845\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings(\n\u001b[1;32m    846\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m    847\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    850\u001b[0m     past_key_values_length\u001b[38;5;241m=\u001b[39mpast_key_values_length,\n\u001b[1;32m    851\u001b[0m )\n\u001b[0;32m--> 852\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    853\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    854\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    855\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    856\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    857\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    858\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    859\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    860\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    861\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    862\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    863\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    864\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    865\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py:527\u001b[0m, in \u001b[0;36mRobertaEncoder.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    518\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mcheckpoint\u001b[38;5;241m.\u001b[39mcheckpoint(\n\u001b[1;32m    519\u001b[0m         create_custom_forward(layer_module),\n\u001b[1;32m    520\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    524\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    525\u001b[0m     )\n\u001b[1;32m    526\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 527\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    528\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    529\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    530\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    531\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    532\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    533\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    534\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    535\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    537\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py:411\u001b[0m, in \u001b[0;36mRobertaLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    399\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    400\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    401\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    408\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[1;32m    409\u001b[0m     \u001b[38;5;66;03m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[39;00m\n\u001b[1;32m    410\u001b[0m     self_attn_past_key_value \u001b[38;5;241m=\u001b[39m past_key_value[:\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 411\u001b[0m     self_attention_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    412\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    413\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    414\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    415\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    416\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mself_attn_past_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    417\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    418\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m self_attention_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    420\u001b[0m     \u001b[38;5;66;03m# if decoder, the last output is tuple of self-attn cache\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py:338\u001b[0m, in \u001b[0;36mRobertaAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    328\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    329\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    330\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    336\u001b[0m     output_attentions: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    337\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[0;32m--> 338\u001b[0m     self_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    339\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    340\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    341\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    342\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    343\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    344\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    345\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    346\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    347\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput(self_outputs[\u001b[38;5;241m0\u001b[39m], hidden_states)\n\u001b[1;32m    348\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (attention_output,) \u001b[38;5;241m+\u001b[39m self_outputs[\u001b[38;5;241m1\u001b[39m:]  \u001b[38;5;66;03m# add attentions if we output them\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py:274\u001b[0m, in \u001b[0;36mRobertaSelfAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    271\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m head_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    272\u001b[0m     attention_probs \u001b[38;5;241m=\u001b[39m attention_probs \u001b[38;5;241m*\u001b[39m head_mask\n\u001b[0;32m--> 274\u001b[0m context_layer \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatmul\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattention_probs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue_layer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    276\u001b[0m context_layer \u001b[38;5;241m=\u001b[39m context_layer\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m)\u001b[38;5;241m.\u001b[39mcontiguous()\n\u001b[1;32m    277\u001b[0m new_context_layer_shape \u001b[38;5;241m=\u001b[39m context_layer\u001b[38;5;241m.\u001b[39msize()[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m+\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mall_head_size,)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "question = \"How do the camera capabilities of the Apple iPhone 12, Samsung Galaxy S21, and Xiaomi Mi 11 compare?\"\n",
    "\n",
    "key_names = name_query_mix_models_inference(question, name_list, print_process=True)\n",
    "\n",
    "print(\"\\n------------------------------------------------\")\n",
    "print(\"Querying local DataBase ......\")\n",
    "for n in key_names:\n",
    "    specs_texts = query_specs_list(n)\n",
    "    print(\"++++++++++++++++++++++++++++++++++++++++++++++++\")\n",
    "    print(n, \" related texts:\")\n",
    "    for text in specs_texts:\n",
    "        print(\"%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\")\n",
    "        print(qa_inference(question, context = text))\n",
    "print(\"------------------------------------------------\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c762d1-8fa3-4f20-8db0-d228dfa0d3e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c5d54aa-ace2-4393-b9ec-4a0e6e7df43c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
